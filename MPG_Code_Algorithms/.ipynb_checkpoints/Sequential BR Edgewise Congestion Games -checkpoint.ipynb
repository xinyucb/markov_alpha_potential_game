{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8d63854",
   "metadata": {},
   "outputs": [],
   "source": [
    "from edgewise_game import EdgewiseCongGame, next_state, get_reward\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import seaborn as sns; sns.set()\n",
    "import statistics\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import copy\n",
    "from itertools import product\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9ee7657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the states and some necessary info\n",
    "N = 8 #number of agents\n",
    "harm = - 100 * N # pentalty for being in bad state\n",
    "\n",
    "all_states =  [i for i in product(range(2), repeat=4)] # all_states = [(0,0,0,0), (0,0,0,1),...]\n",
    "S = len(all_states)\n",
    "\n",
    "game = EdgewiseCongGame(N,1,[[1,-100],[2,-100],[4,-100],[6,-100]])\n",
    "M = game.num_actions \n",
    "D = game.m #number facilities\n",
    "\n",
    "# Dictionary associating each action (value) to an integer (key)\n",
    "act_dic = {}\n",
    "counter = 0\n",
    "for act in game.actions:\n",
    "\tact_dic[counter] = act \n",
    "\tcounter += 1\n",
    "selected_profiles = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "101d02ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_next_state(game, state, actions, kappa):\n",
    "    # state: [0, 1, 0, 1]\n",
    "    acts_from_ints = [act_dic[i] for i in actions]\n",
    "    density = game.get_counts(acts_from_ints)\n",
    "\n",
    "    new_state = [0] * 4\n",
    "    for j in range(D):\n",
    "        new_state[j] = next_state(state[j], density[j], threshold1=N/2, threshold2=N/4, N=N, kappa=kappa)\n",
    "\n",
    "    return tuple(new_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f633ddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pick_action(prob_dist):\n",
    "    # np.random.choice(range(len(prob_dist)), 1, p = prob_dist)[0]\n",
    "    acts = [i for i in range(len(prob_dist))]\n",
    "    action = np.random.choice(acts, 1, p = prob_dist)\n",
    "    return action[0]\n",
    "def entropy(policy,curr_state,player_index):\n",
    "    ent = 0\n",
    "    for a in range(len(policy[curr_state,player_index])):\n",
    "        ent += policy[curr_state,player_index][a]*np.log(policy[curr_state,player_index][a])\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ceec4043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_function(policy, gamma, T,samples, kappa,tau):\n",
    "    \"\"\"\n",
    "    O(num_samples * S * T) \n",
    "    get value function by generating trajectories and calculating the rewards\n",
    "    Vi(s) = sum_{t<T} gamma^t r(t)\n",
    "    \"\"\"\n",
    "    value_fun = {(s,i):0 for s in all_states for i in range(N)}  ### change here | S before means num of states\n",
    "    for k in range(samples):\n",
    "        for state in all_states: ### change here | S before means num of states\n",
    "            curr_state = state\n",
    "            for t in range(T):\n",
    "                actions = [pick_action(policy[curr_state, i]) for i in range(N)]\n",
    "                q = tuple(actions+[curr_state])\n",
    "                # setdefault(key, value): if key exists in di   c, return its original value in dic. Else, add this new key and value into dic \n",
    "                rewards = selected_profiles.setdefault(q, get_reward(game, [act_dic[i] for i in actions], curr_state, kappa))                  \n",
    "                for i in range(N):\n",
    "                    value_fun[state,i] += (gamma**t)*rewards[i] - tau*(gamma**t)*entropy(policy,curr_state,i)\n",
    "                curr_state = sample_next_state(game, curr_state, actions, kappa)\n",
    "    value_fun.update((x,v/samples) for (x,v) in value_fun.items())\n",
    "    return value_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d5b3e619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_function(agent, state, action, policy, gamma, value_fun, samples, kappa,tau,actset):\n",
    "    \"\"\"\n",
    "    Q = r(s, ai) + gamma * V(s)\n",
    "    \"\"\"\n",
    "    tot_reward = 0\n",
    "    for i in range(samples):\n",
    "        actions = [pick_action(policy[state, i]) for i in range(N)]\n",
    "        if actset ==1:\n",
    "            actions[agent] = action\n",
    "            \n",
    "        q = tuple(actions+[state])\n",
    "        rewards = selected_profiles.setdefault(q, get_reward(game, [act_dic[i] for i in actions], state, kappa))                  \n",
    "        tot_reward += rewards[agent] - tau*entropy(policy,state,agent) + gamma*value_fun[sample_next_state(game, state, actions, kappa), agent]\n",
    "    return (tot_reward / samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "27cb9b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_function_one_step_policy(agent, state, policy, policy_one_step , gamma, value_fun, samples, kappa,tau):\n",
    "    \"\"\"\n",
    "    Q = r(s, ai) + gamma * V(s)\n",
    "    \"\"\"\n",
    "    tot_reward = 0\n",
    "    for i in range(samples):\n",
    "        actions = [pick_action(policy[state, i]) for i in range(N)]\n",
    "        q = tuple(actions+[state])\n",
    "        rewards = selected_profiles.setdefault(q, get_reward(game, [act_dic[i] for i in actions], state, kappa))                  \n",
    "        tot_reward += rewards[agent] - tau*entropy(policy,state,agent)- tau*entropy(policy_one_step,state,agent) + gamma*value_fun[sample_next_state(game, state, actions, kappa), agent]\n",
    "    return (tot_reward / samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bf3637b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_accuracy(policy_pi, policy_star):\n",
    "    total_dif = N * [0]\n",
    "    for agent in range(N):\n",
    "        for state in all_states:\n",
    "            total_dif[agent] += np.sum(np.abs((policy_pi[state, agent] - policy_star[state, agent])))\n",
    "\t  # total_dif[agent] += np.sqrt(np.sum((policy_pi[state, agent] - policy_star[state, agent])**2))\n",
    "    return np.sum(total_dif) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a7f27c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87841051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cf6ad896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_br(max_iters, gamma, eta, T, samples,kappa,tau):\n",
    "\n",
    "    policy = {(s,i): [1/M]*M for s in all_states for i in range(N)}\n",
    "    policy_exp = {(s,i): [1/M]*M for s in all_states for i in range(N)}\n",
    "    policy_hist = [copy.deepcopy(policy)]\n",
    "\n",
    "    for t in tqdm.tqdm(range(max_iters)):\n",
    "        b_dist = M * [1]\n",
    "            \n",
    "        grads_exp = np.zeros((N, S, M))\n",
    "        grads_diff = np.zeros((N,S))\n",
    "        grads = np.zeros((N, S, M))\n",
    "        value_fun = value_function(policy, gamma, T, samples, kappa,tau)\n",
    "\t\n",
    "        # Computing the possible improvement for all players \n",
    "        for agent in range(N):\n",
    "            for s in range(S):\n",
    "                for act in range(M):\n",
    "                    st = all_states[s]\n",
    "                    grads_exp[agent, s, act] =  Q_function(agent, st, act, policy, gamma, value_fun, samples, kappa, tau,1)\n",
    "        \n",
    "        for agent in range(N):\n",
    "            for s in range(S):\n",
    "                st = all_states[s]\n",
    "                sum_s_agent = 0\n",
    "                for a in range(M):\n",
    "                    sum_s_agent += np.exp(grads[agent,s,a]/tau)\n",
    "                for a in range(M):\n",
    "                    policy_exp[st,agent][a]= np.exp(grads[agent,s,a]/tau)/sum_s_agent\n",
    "        \n",
    "        ## Finding max improvement player \n",
    "        \n",
    "        for agent in range(N):\n",
    "            for s in range(S):\n",
    "                st = all_states[s]\n",
    "                fac1 = Q_function_one_step_policy(agent, st, policy, policy_exp, gamma, value_fun, samples, kappa, tau)\n",
    "                fac2 = Q_function_one_step_policy(agent, st, policy, policy, gamma, value_fun, samples, kappa, tau)\n",
    "                grads_diff[agent, s] =  fac1-fac2\n",
    "                                            \n",
    "        max_index = np.where(grads_diff==np.max(grads_diff))\n",
    "        agent_max = max_index[0][0]\n",
    "        s_max = max_index[1][0]\n",
    "                \n",
    "        for agent in range(N):\n",
    "            for s in range(S):\n",
    "                st = all_states[s]\n",
    "                sum_s_agent = 0\n",
    "                \n",
    "                if agent == agent_max and s == s_max and grads_diff[agent,s]>=0:\n",
    "                    for a in range(M):\n",
    "                        sum_s_agent += np.exp(grads[agent,s,a]/tau)\n",
    "                    for a in range(M):\n",
    "                        policy[st,agent][a]= np.exp(grads[agent,s,a]/tau)/sum_s_agent\n",
    "        \n",
    "        policy_hist.append(copy.deepcopy(policy))\n",
    "\n",
    "        if policy_accuracy(policy_hist[t], policy_hist[t-1]) < 10e-16:\n",
    "      # if policy_accuracy(policy_hist[t+1], policy_hist[t]) < 10e-16: (it makes a difference, not when t=0 but from t=1 onwards.)\n",
    "            return policy_hist\n",
    "\n",
    "    return policy_hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "860106e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_experiment(runs,iters,eta,T,samples, kappa,tau):\n",
    "    path = \"edgewise_model_results_seq_br/kappa_\"+str(kappa) + \"/\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    densities = np.zeros((S,M))\n",
    "\n",
    "    raw_accuracies = []\n",
    "    for k in tqdm.tqdm(range(runs)):\n",
    "        policy_hist = sequential_br(iters,0.99,eta,T,samples, kappa,tau)\n",
    "        raw_accuracies.append(get_accuracies(policy_hist))\n",
    "\n",
    "        converged_policy = policy_hist[-1]\n",
    "        for i in range(N):\n",
    "            for s in range(S):\n",
    "                st = all_states[s]\n",
    "                densities[s] += converged_policy[st,i]\n",
    "\n",
    "    densities = densities / runs\n",
    "\n",
    "\n",
    "    # Plot Figure 1: trajectories of L1 accuracy\n",
    "    plot_accuracies = np.array(list(itertools.zip_longest(*raw_accuracies, fillvalue=np.nan))).T\n",
    "    clrs = sns.color_palette(\"husl\", 3)\n",
    "    piters = list(range(plot_accuracies.shape[1]))\n",
    "\n",
    "    fig2 = plt.figure(figsize=(6,4))\n",
    "    for i in range(len(plot_accuracies)):\n",
    "        plt.plot(piters, plot_accuracies[i])\n",
    "    plt.grid(linewidth=0.6)\n",
    "    plt.gca().set(xlabel='Iterations',ylabel='L1-accuracy', title='Policy Gradient: agents = {}, runs = {}, $\\eta$ = {}'.format(N, runs,eta))\n",
    "    plt.show()\n",
    "    fig2.savefig(path + 'individual_runs_n{}.png'.format(N),bbox_inches='tight')\n",
    "\n",
    "\n",
    "    # Plot Figure 2: mean and std of L1 accuracy\n",
    "    plot_accuracies = np.nan_to_num(plot_accuracies)\n",
    "    pmean = list(map(statistics.mean, zip(*plot_accuracies)))\n",
    "    pstdv = list(map(statistics.stdev, zip(*plot_accuracies)))\n",
    "\n",
    "    fig1 = plt.figure(figsize=(6,4))\n",
    "    ax = sns.lineplot( pmean, color = clrs[0],label= 'Mean L1-accuracy')\n",
    "    ax.fill_between(piters, np.subtract(pmean,pstdv), np.add(pmean,pstdv), alpha=0.3, facecolor=clrs[0],label=\"1-standard deviation\")\n",
    "    ax.legend()\n",
    "    plt.grid(linewidth=0.6)\n",
    "    plt.gca().set(xlabel='Iterations',ylabel='L1-accuracy', title='Policy Gradient: agents = {}, runs = {}, $\\eta$ = {}'.format(N, runs,eta))\n",
    "    plt.show()\n",
    "    fig1.savefig(path + 'avg_runs_n{}.png'.format(N),bbox_inches='tight')\n",
    "\n",
    "\n",
    "    # Plot Figure 3: Density under different states\n",
    "    fig3, ax = plt.subplots()\n",
    "    index = np.arange(D)\n",
    "    bar_width = 0.25\n",
    "    opacity = 1\n",
    "\n",
    "    id1 = all_states.index((0,0,0,0))\n",
    "    rects1 = plt.bar(index, densities[id1], bar_width,\n",
    "    alpha= .7 * opacity,\n",
    "    color='b',\n",
    "    label= str(all_states[id1]))\n",
    "\n",
    "    id2 =  all_states.index((0,0,0,1))\n",
    "    rects2 = plt.bar(index + bar_width, densities[id2], bar_width,\n",
    "    alpha= opacity,\n",
    "    color='y',\n",
    "    label= str(all_states[id2]))\n",
    "\n",
    "    id3 =  all_states.index((0,1,1,0))\n",
    "    rects2 = plt.bar(index + bar_width + bar_width, densities[id3], bar_width,\n",
    "    alpha= opacity,\n",
    "    color='r',\n",
    "    label= str(all_states[id3]))\n",
    "\n",
    "    plt.gca().set(xlabel='Facility',ylabel='Average number of agents', title='Policy Gradient: agents = {}, runs = {}, $\\eta$ = {}'.format(N,runs,eta))\n",
    "    plt.xticks(index + bar_width/2, ('A', 'B', 'C', 'D'))\n",
    "    plt.legend()\n",
    "    fig3.savefig(path+ 'facilities_n{}.png'.format(N),bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aab73d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "96f36bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\n",
      "  0%|                                                   | 0/200 [00:02<?, ?it/s]\u001b[A\n",
      "  0%|                                                     | 0/2 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'agent_max' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ck/x2wg4sf53j53qt7wxmjl4p4w0000gn/T/ipykernel_87828/3405010335.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# If you want deterministic transition, set kappa=\"deterministic\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mruns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkappa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"deterministic\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfull_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkappa\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/ck/x2wg4sf53j53qt7wxmjl4p4w0000gn/T/ipykernel_87828/2402917017.py\u001b[0m in \u001b[0;36mfull_experiment\u001b[0;34m(runs, iters, eta, T, samples, kappa, tau)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mraw_accuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mpolicy_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequential_br\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkappa\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mraw_accuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_accuracies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_hist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/ck/x2wg4sf53j53qt7wxmjl4p4w0000gn/T/ipykernel_87828/1590877134.py\u001b[0m in \u001b[0;36msequential_br\u001b[0;34m(max_iters, gamma, eta, T, samples, kappa, tau)\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0msum_s_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0magent_max\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0ms_max\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgrads_diff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                         \u001b[0msum_s_agent\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'agent_max' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# kappa is the parameter in logisitic function.\n",
    "# If you want deterministic transition, set kappa=\"deterministic\"\n",
    "runs,iters,eta,T,samples, kappa, tau  = 2, 200, 0.01,20,10,\"deterministic\", 0.1\n",
    "full_experiment(runs,iters,eta,T,samples, kappa,tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "55e99d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c54c05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "A[0,0,0] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "18020716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ec11123b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t= np.where(A==np.max(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1576ee91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2231f257",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
