{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Sep 25 16:01:04 2023\n",
    "\n",
    "@author: xinyuli\n",
    "\"\"\"\n",
    "\n",
    "#import cvxpy as cp \n",
    "from CongestionGame import congestion_game\n",
    "from utils import *\n",
    "from routing_game import *\n",
    "from framework.game import *\n",
    "from framework.utils import *\n",
    "import copy\n",
    "import math\n",
    "import typing\n",
    "import itertools\n",
    "from tqdm import tqdm \n",
    "\n",
    "def random_policy_single_player(i, S, M):\n",
    "    dic = {}\n",
    "    Ai = ActionSet(i, [Action(i, value=j) for j in range(M)]) \n",
    "\n",
    "    for s in S:\n",
    "        # Generate a list of random numbers that sum to 1\n",
    "        random_numbers = [random.uniform(0, 1) for _ in range(M)]\n",
    "        total_sum = sum(random_numbers)\n",
    "\n",
    "        # Normalize the numbers to ensure they sum to 1\n",
    "        normalized_numbers = [num / total_sum for num in random_numbers]\n",
    "\n",
    "        # Print the list of random numbers that sum to 1\n",
    "        cnt = 0\n",
    "        for ai in Ai:\n",
    "            dic[s,ai] = normalized_numbers[cnt]\n",
    "            cnt += 1\n",
    "    return dic\n",
    "\n",
    "\n",
    "def generate_random_pi(I, S, A, M):\n",
    "    single_player_policies = {}\n",
    "    for i in I:\n",
    "        single_player_policies[i] = random_policy_single_player(i, S, M)\n",
    "\n",
    "    def generate_single_player_random_policy(i, s, a):\n",
    "        return single_player_policies[i][s,a]\n",
    "    pi = dict()\n",
    "    for i in I:\n",
    "        pi[i] = Policy(S, A[i], lambda s, a: generate_single_player_random_policy(i, s, a))\n",
    "    pi = JointPolicy(pi)\n",
    "    return pi\n",
    "\n",
    "\n",
    "def modify_one_players_policy(i, pi, pi_prime, I, S, A):\n",
    "    new_pi = dict()\n",
    "    for j in I:\n",
    "        if j != i:\n",
    "            new_pi[j] = Policy(S, A[j], lambda s, ai: pi[j][s, ai])\n",
    "        else:\n",
    "            new_pi[j] = Policy(S, A[j], lambda s, ai: pi_prime[j][s, ai])\n",
    "    new_pi = JointPolicy(new_pi)\n",
    "    return new_pi\n",
    "\n",
    "def gi(i, pi, pi_prime, phi, R, P, S, list_S, A, delta, mu):\n",
    "    pi_a_i = pi[i]\n",
    "    pi_minus_i = pi.minus(i)\n",
    "    d_pi_a = construct_d_pi(i, pi_a_i, pi_minus_i, P, list_S, A, delta, mu)\n",
    "\n",
    "    pi_b_i = pi_prime[i]\n",
    "    d_pi_b = construct_d_pi(i, pi_b_i, pi_minus_i, P, list_S, A, delta, mu)\n",
    "\n",
    "    return abs(sum((d_pi_b[idxs,idxa]-d_pi_a[idxs,idxa])*(phi[s][a]-R.get_reward(i, s, a))\n",
    "        for idxs, s in enumerate(S) for idxa, a in enumerate(A)))\n",
    "\n",
    "\n",
    "def g(pi, pi_prime,x, I, phi, R, P, S, list_S, A, delta, mu):\n",
    "    max_val = 0\n",
    "    for i in I:\n",
    "        g_i = gi(i, pi, pi_prime, phi, R, P, S, list_S, A, delta, mu)\n",
    "        if g_i > max_val:\n",
    "            max_val = g_i\n",
    "    return max_val -x\n",
    "\n",
    "def find_best_potential_function_randomized(N, M, U, m, b, lambda_1, lambda_2, delta,\n",
    "               K, tau, alpha_r, beta_r, common_interest, strategy_independent):\n",
    "    game = congestion_game(N=N, M=M, U=U, m=m, b=b, lambda_1=lambda_1, lambda_2=lambda_2, delta=delta,\n",
    "                                   common_interest=common_interest, strategy_independent_transitions=strategy_independent)\n",
    " \n",
    "    I = game.I  # player set\n",
    "    S = game.S  # state set\n",
    "    A = game.A  # action profile set\n",
    "    mu = game.mu  # initial state distribution\n",
    "    P = game.P  # probability transition kernel\n",
    "    R = game.R  # reward function\n",
    "    delta = game.delta  # discount factor\n",
    "    list_S = list(S)\n",
    "\n",
    "    phi = {s: {a: game.reward_facility(s, a) for a in A} for s in S}\n",
    "\n",
    "    def gamma_(n):\n",
    "        return  1/(5* n) \n",
    "\n",
    "    def delta_(n):\n",
    "        return (n)**0.45 \n",
    "\n",
    "\n",
    "\n",
    "    T = 100\n",
    "    X = 1\n",
    "    for n in range(T+1):\n",
    "        pi = generate_random_pi(I, S, A, M)\n",
    "        pi_prime = generate_random_pi(I, S, A, M)\n",
    "        g_val = g(pi, pi_prime, X, I, phi, R, P, S, list_S, A, delta, mu)\n",
    "        X_new = X - gamma_(n+1) * (1 - delta_(n+1) * (g_val > 0))\n",
    "\n",
    "        if X == 0:\n",
    "            print(\"n =\",n, \", X =\", X, \", gamma =\", gamma_(n+1),\n",
    "                    \", g =\", g_val )\n",
    "            print(\"g:\", g_val)\n",
    "            #break\n",
    "        if X_new < 0: X_new = 0\n",
    "        \n",
    "        X = X_new\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim to solve the following semi-infinite programming problem:\n",
    "$$ \n",
    "\\begin{array}{lll}\n",
    "&\\min_x &x\\\\\n",
    "&s.t. & x\\geq |\\sum_s \\sum_a (d^{\\mu}(\\pi) -d^{\\mu})(\\pi_i', \\pi_{-i}) (u_i - \\phi)(s,a) )|, \\quad \\forall i \\in I\n",
    "\\end{array}\n",
    "$$\n",
    "where $$\\phi(s,a) = \\sum_{j \\in M}\\sum_{k=1}^{\\#(a,j)} c_j(k, s).$$\n",
    "First define $g$ to be\n",
    "$$\n",
    "g(x, \\pi, \\pi') = \\max_i \\left|\\sum_{s, a}  (d^\\mu(\\pi) - d^\\mu(\\pi_i', \\pi_{-i}))(u_i - \\phi)(s,a) \\right| - x\n",
    "$$\n",
    "and let $h$ be\n",
    "$$h(g) = \\max\\{0, g\\}$$\n",
    "\n",
    "According to the update rule (14) in Tadic et al,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X_{n+1} &= X_n - \\gamma_{n+1} \\nabla f(X_n) - \\gamma_{n+1}  \\delta_{n+1}h'(g) \\nabla_x g(x, \\pi, \\pi') \\\\ \n",
    "\\end{aligned}\n",
    "$$\n",
    "which is equivalent to \n",
    "$$ X_{n+1} = X_n - \\gamma_{n+1}  + \\gamma_{n+1}  \\delta_{n+1} 1_{\\{ g > 0\\}}  $$\n",
    "\n",
    "\n",
    "In the paper, it requires $\\gamma_n = n^{-c}$ with $c\\in(-0.5, 1]$.\n",
    "Moreover, $\\gamma_n >0, \\sum \\gamma_n = \\infty, \\sum \\gamma_n^2 \\delta_n^2 < \\infty$\n",
    "\n",
    "$\\delta_n$ is an increase sequence with $\\lim \\delta_n = \\infty$.\n",
    "\n",
    "Our choice: $\\gamma_n \\propto \\frac{1}{n}, \\delta_n = n^{0.45}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 83 , X = 0 , gamma = 0.002380952380952381 , g = 1.8735013540549517e-16\n",
      "g: 1.8735013540549517e-16\n",
      "n = 91 , X = 0 , gamma = 0.002173913043478261 , g = 5.342948306008566e-16\n",
      "g: 5.342948306008566e-16\n",
      "n = 99 , X = 0 , gamma = 0.002 , g = 1.3877787807814457e-16\n",
      "g: 1.3877787807814457e-16\n"
     ]
    }
   ],
   "source": [
    "N = 3\n",
    "M = 2\n",
    "U = 1\n",
    "m = [2, 4]\n",
    "b = [9, 16]\n",
    "lambda_1 = 0.8\n",
    "lambda_2 = 0.2\n",
    "delta = 0.8\n",
    "K = int(1e5)\n",
    "tau = 1e-6\n",
    "alpha_r = 0.5\n",
    "beta_r = 1\n",
    "common_interest =  False\n",
    "strategy_independent = False\n",
    "find_best_potential_function_randomized(N, M, U, m, b, lambda_1, lambda_2, delta,\n",
    "               K, tau, alpha_r, beta_r, common_interest, strategy_independent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 83 , X = 0 , gamma = 0.002380952380952381 , g = 2.05911676598447e-15\n",
      "g: 2.05911676598447e-15\n",
      "n = 91 , X = 0 , gamma = 0.002173913043478261 , g = 1.2212453270876722e-15\n",
      "g: 1.2212453270876722e-15\n",
      "n = 99 , X = 0 , gamma = 0.002 , g = 1.0234868508263162e-15\n",
      "g: 1.0234868508263162e-15\n"
     ]
    }
   ],
   "source": [
    "N = 6\n",
    "M = 2\n",
    "U = 1\n",
    "m = [2, 4]\n",
    "b = [0, 0]\n",
    "lambda_1 = 1\n",
    "lambda_2 = 0\n",
    "delta = 0.8\n",
    "K = int(1e5)\n",
    "tau = 1e-6\n",
    "alpha_r = 0.5\n",
    "beta_r = 1\n",
    "common_interest =  False\n",
    "strategy_independent = False\n",
    "find_best_potential_function_randomized(N, M, U, m, b, lambda_1, lambda_2, delta,\n",
    "               K, tau, alpha_r, beta_r, common_interest, strategy_independent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check\n",
    "$$max_{i,j} | \\sum_s\\sum_a (d^\\mu(\\pi)-d^\\mu(\\pi_i’,\\pi_j’,\\pi_{-ij}))(u_i-\\phi) |$$ \n",
    "is not very close to 0 to make sure the code does not have bug\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_two_players_policy(i, k, pi, pi_prime, I, S, A):\n",
    "    new_pi = dict()\n",
    "    for j in I:\n",
    "        if j != i or j != k:\n",
    "            new_pi[j] = Policy(S, A[j], lambda s, ai: pi[j][s, ai])\n",
    "        else:\n",
    "            new_pi[j] = Policy(S, A[j], lambda s, ai: pi_prime[j][s, ai])\n",
    "    new_pi = JointPolicy(new_pi)\n",
    "    return new_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "game = congestion_game(N=N, M=M, U=U, m=m, b=b, lambda_1=lambda_1, lambda_2=lambda_2, delta=delta,\n",
    "                                common_interest=common_interest, strategy_independent_transitions=strategy_independent)\n",
    "\n",
    "I = game.I  # player set\n",
    "S = game.S  # state set\n",
    "A = game.A  # action profile set\n",
    "mu = game.mu  # initial state distribution\n",
    "P = game.P  # probability transition kernel\n",
    "R = game.R  # reward function\n",
    "delta = game.delta  # discount factor\n",
    "list_S = list(S)\n",
    "\n",
    "phi = {s: {a: game.reward_facility(s, a) for a in A} for s in S}\n",
    "\n",
    "\n",
    "# i is the first player, k is the last player\n",
    "for i in I:\n",
    "    print(i)\n",
    "    break\n",
    "for k in I:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_pi_dict(I, S, A, M):\n",
    "    single_player_policies_a= {}\n",
    "    for i in I:\n",
    "        single_player_policies_a[i] = random_policy_single_player(i, S, M)\n",
    "\n",
    "    return single_player_policies_a\n",
    "\n",
    "\n",
    "\n",
    "def generate_single_player_random_policy(single_player_policies, i, s, a):\n",
    "    return single_player_policies[i][s,a]\n",
    "\n",
    "\n",
    "\n",
    "def create_pi_ik(single_player_policies_a, single_player_policies_b, i, k):\n",
    "    \"\"\"\n",
    "    pi_i and pi_k are from policy b\n",
    "    others from policy a\n",
    "    \"\"\"\n",
    "    pi = dict()\n",
    "    for j in I:\n",
    "        if j !=i and j!=k:\n",
    "            pi[j] = Policy(S, A[j], \\\n",
    "                lambda s, a: generate_single_player_random_policy(single_player_policies_a, j, s, a))\n",
    "        else:\n",
    "            pi[j] = Policy(S, A[j], \\\n",
    "                lambda s, a: generate_single_player_random_policy(single_player_policies_b, j, s, a))\n",
    "\n",
    "    pi = JointPolicy(pi)\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_player_policies_a = generate_random_pi_dict(I, S, A, M)\n",
    "single_player_policies_b = generate_random_pi_dict(I, S, A, M)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gi2(i, k, pi, pi_prime, phi, R, P, S, list_S, A, delta, mu):\n",
    "    pi_ik_prime = create_pi_ik(single_player_policies_a, single_player_policies_b, i, k)\n",
    "    pi_ik = create_pi_ik(single_player_policies_a, single_player_policies_a, i, k)\n",
    "\n",
    "    pi_a_i = pi_ik[i]\n",
    "\n",
    "    pi_minus_i = pi.minus(i)\n",
    "    #d^mu(pi)\n",
    "    d_pi_a = construct_d_pi(i, pi_a_i, pi_minus_i, P, list_S, A, delta, mu)\n",
    "\n",
    "    \n",
    "    pi_b_i = pi_ik_prime[i]\n",
    "    pi_minus_i_b = pi_ik_prime.minus(i)\n",
    "    #d^mu(pi'{i}, pi'{j}, pi{-i, -j})\n",
    "    d_pi_b = construct_d_pi(i, pi_b_i, pi_minus_i_b, P, list_S, A, delta, mu)\n",
    "    return abs(sum((d_pi_b[idxs,idxa]-d_pi_a[idxs,idxa])*(phi[s][a]-R.get_reward(i, s, a))\n",
    "        for idxs, s in enumerate(S) for idxa, a in enumerate(A)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$ g_{i} = | \\sum_s\\sum_a (d^\\mu(\\pi)-d^\\mu(\\pi_i',\\pi_{-i}))(u_i-\\phi) | $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3114509478384662e-15"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi = generate_random_pi(I, S, A, M)\n",
    "pi_prime = generate_random_pi(I, S, A, M)\n",
    "\n",
    "gi(i, pi, pi_prime, phi, R, P, S, list_S, A, delta, mu)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$ g_{ik} = | \\sum_s\\sum_a (d^\\mu(\\pi)-d^\\mu(\\pi_i',\\pi_k',\\pi_{-ik}))(u_i-\\phi) | $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2231770513304578"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gi2(i,  k, pi, pi_prime, phi, R, P, S, list_S, A, delta, mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amazon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
